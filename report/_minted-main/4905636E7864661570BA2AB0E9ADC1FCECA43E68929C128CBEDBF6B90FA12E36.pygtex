\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{n}{sys}\PYG{o}{.}\PYG{n}{path}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}..\PYGZsq{}}\PYG{p}{)}

\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{defaultdict}
\PYG{k+kn}{from} \PYG{n+nn}{copy} \PYG{k+kn}{import} \PYG{n}{deepcopy}
\PYG{k+kn}{import} \PYG{n+nn}{itertools}
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{import} \PYG{n+nn}{logging}
\PYG{k+kn}{import} \PYG{n+nn}{math}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k+kn}{import} \PYG{n+nn}{time}

\PYG{c+c1}{\PYGZsh{} from MCTS import MonteCarloTreeSearch}
\PYG{k+kn}{from} \PYG{n+nn}{MCTS.mcts} \PYG{k+kn}{import} \PYG{n}{decode\PYGZus{}tree}
\PYG{k+kn}{from} \PYG{n+nn}{MCTS2.mcts} \PYG{k+kn}{import} \PYG{n}{MCTS}
\PYG{k+kn}{from} \PYG{n+nn}{quarto.objects} \PYG{k+kn}{import} \PYG{n}{Quarto}
\PYG{k+kn}{from} \PYG{n+nn}{lib.players} \PYG{k+kn}{import} \PYG{n}{Player}\PYG{p}{,} \PYG{n}{RandomPlayer}
\PYG{k+kn}{from} \PYG{n+nn}{lib.isomorphic} \PYG{k+kn}{import} \PYG{n}{BoardTransforms}

\PYG{k+kn}{import} \PYG{n+nn}{tqdm}
\PYG{n}{logging}\PYG{o}{.}\PYG{n}{basicConfig}\PYG{p}{(}\PYG{n}{level}\PYG{o}{=}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{DEBUG}\PYG{p}{)}


\PYG{k}{class} \PYG{n+nc}{QLearningPlayer}\PYG{p}{(}\PYG{n}{Player}\PYG{p}{):}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{board}\PYG{p}{:} \PYG{n}{Quarto} \PYG{o}{=} \PYG{n}{Quarto}\PYG{p}{(),} \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{n}{tree}\PYG{p}{:} \PYG{n}{MCTS} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{):}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{epsilon}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha} \PYG{o}{=} \PYG{n}{alpha}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{board} \PYG{o}{=} \PYG{n}{board}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MAX\PYGZus{}PIECES} \PYG{o}{=} \PYG{l+m+mi}{16}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{BOARD\PYGZus{}SIDE} \PYG{o}{=} \PYG{l+m+mi}{4}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{tree} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} load the pre\PYGZhy{}initalised tree}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree} \PYG{o}{=} \PYG{n}{tree}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{set\PYGZus{}board}\PYG{p}{(}\PYG{n}{board}\PYG{p}{)}

        \PYG{k}{else}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} load new tree}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree} \PYG{o}{=} \PYG{n}{MCTS}\PYG{p}{(}\PYG{n}{board}\PYG{o}{=}\PYG{n}{board}\PYG{p}{)}

        \PYG{n+nb}{super}\PYG{p}{()}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{board}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{clear\PYGZus{}and\PYGZus{}set\PYGZus{}current\PYGZus{}state}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{:} \PYG{n}{Quarto}\PYG{p}{):}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state} \PYG{o}{=} \PYG{n}{state}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree} \PYG{o}{=} \PYG{n}{MCTS}\PYG{p}{(}\PYG{n}{board}\PYG{o}{=}\PYG{n}{state}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{reduce\PYGZus{}normal\PYGZus{}form}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{:} \PYG{n}{Quarto}\PYG{p}{):}
        \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{l+s+sd}{        Reduce the Quarto board to normal form (i.e. the board is symmetric)}
\PYG{l+s+sd}{        \PYGZsq{}\PYGZsq{}\PYGZsq{}}
        \PYG{c+c1}{\PYGZsh{} NOT IMPLEMENTED for now, just return the board}
        \PYG{k}{return} \PYG{n}{state}

    \PYG{k}{def} \PYG{n+nf}{hash\PYGZus{}state\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{:} \PYG{n}{Quarto}\PYG{p}{,} \PYG{n}{action}\PYG{p}{):}
        \PYG{c+c1}{\PYGZsh{} reduce to normal form before saving to Q table}
        \PYG{k}{return} \PYG{n}{state}\PYG{o}{.}\PYG{n}{board\PYGZus{}to\PYGZus{}string}\PYG{p}{()} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}||\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{state}\PYG{o}{.}\PYG{n}{get\PYGZus{}selected\PYGZus{}piece}\PYG{p}{())} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}||\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{):}
        \PYG{c+c1}{\PYGZsh{} check possible transforms first (really really slow)}
        \PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{val} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{o}{.}\PYG{n}{items}\PYG{p}{():}
            \PYG{k}{if} \PYG{n}{BoardTransforms}\PYG{o}{.}\PYG{n}{compare\PYGZus{}boards}\PYG{p}{(}\PYG{n}{state}\PYG{o}{.}\PYG{n}{state\PYGZus{}as\PYGZus{}array}\PYG{p}{(),} \PYG{n}{state}\PYG{o}{.}\PYG{n}{string\PYGZus{}to\PYGZus{}board}\PYG{p}{(}\PYG{n}{key}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}||\PYGZsq{}}\PYG{p}{)[}\PYG{l+m+mi}{0}\PYG{p}{])):}
                \PYG{k}{return} \PYG{n}{val}

        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash\PYGZus{}state\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} used to determine if state exists in Q table}
            \PYG{c+c1}{\PYGZsh{} if None, then go to MCTS}
            \PYG{k}{return} \PYG{k+kc}{None}

        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash\PYGZus{}state\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)]}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}Q\PYGZus{}for\PYGZus{}state}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{):}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash\PYGZus{}state\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{)} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{:}
            \PYG{k}{return} \PYG{k+kc}{None}
        \PYG{k}{return} \PYG{p}{[}\PYG{n}{i} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q} \PYG{k}{if} \PYG{n}{i}\PYG{o}{.}\PYG{n}{startswith}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{state}\PYG{p}{))]}

    \PYG{k}{def} \PYG{n+nf}{set\PYGZus{}Q}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{value}\PYG{p}{):}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hash\PYGZus{}state\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)]} \PYG{o}{=} \PYG{n}{value}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}possible\PYGZus{}actions}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{:} \PYG{n}{Quarto}\PYG{p}{):}
        \PYG{n}{actions} \PYG{o}{=} \PYG{p}{[]}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{BOARD\PYGZus{}SIDE}\PYG{p}{):}
            \PYG{k}{for} \PYG{n}{j} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{BOARD\PYGZus{}SIDE}\PYG{p}{):}
                \PYG{k}{for} \PYG{n}{piece} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{MAX\PYGZus{}PIECES}\PYG{p}{):}
                    \PYG{k}{if} \PYG{n}{state}\PYG{o}{.}\PYG{n}{check\PYGZus{}if\PYGZus{}move\PYGZus{}valid}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{board}\PYG{o}{.}\PYG{n}{get\PYGZus{}selected\PYGZus{}piece}\PYG{p}{(),} \PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{,} \PYG{n}{piece}\PYG{p}{):}
                        \PYG{n}{actions}\PYG{o}{.}\PYG{n}{append}\PYG{p}{((}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{,} \PYG{n}{piece}\PYG{p}{))}

        \PYG{k}{return} \PYG{n}{actions}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}max\PYGZus{}Q}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{):}
        \PYG{n}{max\PYGZus{}Q} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{math}\PYG{o}{.}\PYG{n}{inf}
        \PYG{k}{for} \PYG{n}{action} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}possible\PYGZus{}actions}\PYG{p}{(}\PYG{n}{state}\PYG{p}{):}
            \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
                \PYG{n}{Q\PYGZus{}val} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)}
                \PYG{n}{max\PYGZus{}Q} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{max\PYGZus{}Q}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{))}
        \PYG{k}{return} \PYG{n}{max\PYGZus{}Q}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}testing\PYGZsq{}}\PYG{p}{):}
        \PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{}}
\PYG{l+s+sd}{        If state, action pair not in Q, go to Monte Carlo Tree Search to find best action}
\PYG{l+s+sd}{        \PYGZsq{}\PYGZsq{}\PYGZsq{}}
        \PYG{k}{if} \PYG{n}{mode} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}training\PYGZsq{}}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} exploration through epsilon greedy}
            \PYG{c+c1}{\PYGZsh{} look for good moves through Monte Carlo Tree Search}
            \PYG{k}{if} \PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{()} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{:}
                \PYG{c+c1}{\PYGZsh{} for i in range(10):}
                \PYG{c+c1}{\PYGZsh{}     self.tree.do\PYGZus{}rollout(state)}
                \PYG{n}{best\PYGZus{}action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{place\PYGZus{}piece}\PYG{p}{()}
                \PYG{k}{return} \PYG{n}{best\PYGZus{}action}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{c+c1}{\PYGZsh{} look in the q table for the best action}
                \PYG{n}{expected\PYGZus{}score} \PYG{o}{=} \PYG{l+m+mi}{0}
                \PYG{n}{best\PYGZus{}action} \PYG{o}{=} \PYG{k+kc}{None}
                \PYG{k}{for} \PYG{n}{action} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}possible\PYGZus{}actions}\PYG{p}{(}\PYG{n}{state}\PYG{p}{):}
                    \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{expected\PYGZus{}score} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{):}
                        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}found in Q table\PYGZsq{}}\PYG{p}{)}
                        \PYG{n}{expected\PYGZus{}score} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)}
                        \PYG{n}{best\PYGZus{}action} \PYG{o}{=} \PYG{n}{action}
                \PYG{c+c1}{\PYGZsh{} go to Monte Carlo Tree Search if no suitable action found in Q table}
                \PYG{k}{if} \PYG{n}{best\PYGZus{}action} \PYG{o+ow}{is} \PYG{k+kc}{None} \PYG{o+ow}{or} \PYG{n}{expected\PYGZus{}score} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n}{logging}\PYG{o}{.}\PYG{n}{debug}\PYG{p}{(}
                        \PYG{l+s+s1}{\PYGZsq{}No suitable action found in Q table, going to Monte Carlo Tree Search\PYGZsq{}}\PYG{p}{)}
                    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{):}
                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{do\PYGZus{}rollout}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
                    \PYG{n}{best\PYGZus{}action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{place\PYGZus{}piece}\PYG{p}{()}
                \PYG{k}{else}\PYG{p}{:}
                    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}found in Q table\PYGZsq{}}\PYG{p}{)}

                \PYG{k}{return} \PYG{n}{best\PYGZus{}action}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} in test mode, use the Q table to find the best action}
            \PYG{c+c1}{\PYGZsh{} only go to Monte Carlo Tree Search if no suitable action found in Q table}
            \PYG{n}{expected\PYGZus{}score} \PYG{o}{=} \PYG{l+m+mi}{0}
            \PYG{n}{best\PYGZus{}action} \PYG{o}{=} \PYG{k+kc}{None}
            \PYG{k}{for} \PYG{n}{action} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}possible\PYGZus{}actions}\PYG{p}{(}\PYG{n}{state}\PYG{p}{):}
                \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{expected\PYGZus{}score} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{):}
                    \PYG{n}{expected\PYGZus{}score} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)}
                    \PYG{n}{best\PYGZus{}action} \PYG{o}{=} \PYG{n}{action}
            \PYG{c+c1}{\PYGZsh{} go to Monte Carlo Tree Search if no suitable action found in Q table}
            \PYG{k}{if} \PYG{n}{best\PYGZus{}action} \PYG{o+ow}{is} \PYG{k+kc}{None} \PYG{o+ow}{or} \PYG{n}{expected\PYGZus{}score} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{n}{logging}\PYG{o}{.}\PYG{n}{debug}\PYG{p}{(}
                    \PYG{l+s+s1}{\PYGZsq{}No suitable action found in Q table, going to Monte Carlo Tree Search\PYGZsq{}}\PYG{p}{)}
                \PYG{c+c1}{\PYGZsh{} for i in range(20):}
                \PYG{c+c1}{\PYGZsh{}     print(\PYGZsq{}doing rollout\PYGZsq{})}
                \PYG{c+c1}{\PYGZsh{}     self.tree.do\PYGZus{}rollout(state)}
                \PYG{n}{best\PYGZus{}action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{place\PYGZus{}piece}\PYG{p}{()}
            \PYG{k}{return} \PYG{n}{best\PYGZus{}action}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}Q}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{):}
        \PYG{n}{Q\PYGZus{}val} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{Q\PYGZus{}val} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{Q\PYGZus{}val} \PYG{o}{=} \PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.01}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{set\PYGZus{}Q}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{Q\PYGZus{}val} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha} \PYG{o}{*}
                    \PYG{p}{(}\PYG{n}{reward} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}max\PYGZus{}Q}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{Q\PYGZus{}val}\PYG{p}{))}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{iterations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{):}
        \PYG{c+c1}{\PYGZsh{} 1. Use the Q\PYGZhy{}function to initialize the value of each state\PYGZhy{}action pair, Q(s, a) = 0.}
        \PYG{c+c1}{\PYGZsh{} automatically done through defaultdict}

        \PYG{c+c1}{\PYGZsh{} Choose an action using MCTS}
        \PYG{n}{wins} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{tries} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{agent\PYGZus{}decision\PYGZus{}times} \PYG{o}{=} \PYG{p}{[]}

        \PYG{n}{progress\PYGZus{}bar} \PYG{o}{=} \PYG{n}{tqdm}\PYG{o}{.}\PYG{n}{tqdm}\PYG{p}{(}\PYG{n}{total}\PYG{o}{=}\PYG{n}{iterations}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{iterations}\PYG{p}{):}
            \PYG{n}{board} \PYG{o}{=} \PYG{n}{Quarto}\PYG{p}{()}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{board} \PYG{o}{=} \PYG{n}{board}
            \PYG{n}{random\PYGZus{}player} \PYG{o}{=} \PYG{n}{RandomPlayer}\PYG{p}{(}\PYG{n}{board}\PYG{p}{)}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{set\PYGZus{}board}\PYG{p}{(}\PYG{n}{board}\PYG{p}{)}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state} \PYG{o}{=} \PYG{n}{board}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}state} \PYG{o}{=} \PYG{k+kc}{None}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}action} \PYG{o}{=} \PYG{k+kc}{None}
            \PYG{n}{player} \PYG{o}{=} \PYG{l+m+mi}{1}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{switch\PYGZus{}player}\PYG{p}{()}
            \PYG{n}{selected\PYGZus{}piece} \PYG{o}{=} \PYG{n}{random\PYGZus{}player}\PYG{o}{.}\PYG{n}{choose\PYGZus{}piece}\PYG{p}{()}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{set\PYGZus{}selected\PYGZus{}piece}\PYG{p}{(}\PYG{n}{selected\PYGZus{}piece}\PYG{p}{)}
            \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
                \PYG{n}{reward} \PYG{o}{=} \PYG{l+m+mi}{0}
                \PYG{k}{if} \PYG{n}{player} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{c+c1}{\PYGZsh{} QL\PYGZhy{}MCTS moves here}
                    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}QL\PYGZhy{}MCTS moves here\PYGZsq{}}\PYG{p}{)}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}state} \PYG{o}{=} \PYG{n}{deepcopy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{p}{)}
                    \PYG{n}{logging}\PYG{o}{.}\PYG{n}{debug}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}Piece to place: \PYGZdq{}}\PYG{p}{,}
                                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{get\PYGZus{}selected\PYGZus{}piece}\PYG{p}{())}
                    \PYG{n}{logging}\PYG{o}{.}\PYG{n}{debug}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}Board: \PYGZdq{}}\PYG{p}{)}
                    \PYG{n}{logging}\PYG{o}{.}\PYG{n}{debug}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{state\PYGZus{}as\PYGZus{}array}\PYG{p}{())}
                    \PYG{n}{time\PYGZus{}start} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{()}
                    \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{p}{)}
                    \PYG{n}{next\PYGZus{}piece} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{choose\PYGZus{}piece}\PYG{p}{()}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}action} \PYG{o}{=} \PYG{p}{(}\PYG{n}{action}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{n}{action}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{],} \PYG{n}{next\PYGZus{}piece}\PYG{p}{)}
                    \PYG{n}{time\PYGZus{}end} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{()}
                    \PYG{n}{agent\PYGZus{}decision\PYGZus{}times}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{time\PYGZus{}end} \PYG{o}{\PYGZhy{}} \PYG{n}{time\PYGZus{}start}\PYG{p}{)}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{select}\PYG{p}{(}\PYG{n}{selected\PYGZus{}piece}\PYG{p}{)}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{place}\PYG{p}{(}\PYG{n}{action}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{n}{action}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{])}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{set\PYGZus{}selected\PYGZus{}piece}\PYG{p}{(}\PYG{n}{next\PYGZus{}piece}\PYG{p}{)}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{switch\PYGZus{}player}\PYG{p}{()}
                    \PYG{n}{player} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{player}

                \PYG{k}{else}\PYG{p}{:}
                    \PYG{c+c1}{\PYGZsh{} Random moves here}
                    \PYG{n}{action} \PYG{o}{=} \PYG{n}{random\PYGZus{}player}\PYG{o}{.}\PYG{n}{place\PYGZus{}piece}\PYG{p}{()}
                    \PYG{n}{next\PYGZus{}piece} \PYG{o}{=} \PYG{n}{random\PYGZus{}player}\PYG{o}{.}\PYG{n}{choose\PYGZus{}piece}\PYG{p}{()}
                    \PYG{k}{while} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{board}\PYG{o}{.}\PYG{n}{check\PYGZus{}if\PYGZus{}move\PYGZus{}valid}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{board}\PYG{o}{.}\PYG{n}{get\PYGZus{}selected\PYGZus{}piece}\PYG{p}{(),} \PYG{n}{action}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{n}{action}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{],} \PYG{n}{next\PYGZus{}piece}\PYG{p}{)} \PYG{o+ow}{is} \PYG{k+kc}{False}\PYG{p}{:}
                        \PYG{n}{action} \PYG{o}{=} \PYG{n}{random\PYGZus{}player}\PYG{o}{.}\PYG{n}{place\PYGZus{}piece}\PYG{p}{()}
                        \PYG{n}{next\PYGZus{}piece} \PYG{o}{=} \PYG{n}{random\PYGZus{}player}\PYG{o}{.}\PYG{n}{choose\PYGZus{}piece}\PYG{p}{()}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{select}\PYG{p}{(}
                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{get\PYGZus{}selected\PYGZus{}piece}\PYG{p}{())}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{place}\PYG{p}{(}\PYG{n}{action}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{],} \PYG{n}{action}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{])}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{set\PYGZus{}selected\PYGZus{}piece}\PYG{p}{(}\PYG{n}{next\PYGZus{}piece}\PYG{p}{)}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{switch\PYGZus{}player}\PYG{p}{()}
                    \PYG{n}{player} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{player}

                \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{check\PYGZus{}is\PYGZus{}game\PYGZus{}over}\PYG{p}{():}
                    \PYG{k}{if} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{o}{.}\PYG{n}{check\PYGZus{}winner}\PYG{p}{()} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
                        \PYG{n}{logging}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}QL\PYGZhy{}MCTS won\PYGZsq{}}\PYG{p}{)}
                        \PYG{n}{reward} \PYG{o}{=} \PYG{l+m+mi}{1}
                        \PYG{n}{wins} \PYG{o}{+=} \PYG{l+m+mi}{1}
                    \PYG{k}{else}\PYG{p}{:}
                        \PYG{n}{logging}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}Random won\PYGZsq{}}\PYG{p}{)}
                        \PYG{n}{reward} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}Q}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}state}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}action}\PYG{p}{,}
                                    \PYG{n}{reward}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{p}{)}
                    \PYG{k}{break}
                \PYG{k}{else}\PYG{p}{:}
                    \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}state} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}Q}\PYG{p}{(}
                            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}state}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{previous\PYGZus{}action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{current\PYGZus{}state}\PYG{p}{)}

            \PYG{n}{tries} \PYG{o}{+=} \PYG{l+m+mi}{1}
            \PYG{k}{if} \PYG{n}{i} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{10} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{n}{logging}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}Iteration }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
                \PYG{n}{logging}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}Wins: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{wins}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
                \PYG{n}{logging}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}Tries: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{tries}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
                \PYG{n}{logging}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}Win rate: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{wins}\PYG{o}{/}\PYG{n}{tries}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
                \PYG{n}{wins} \PYG{o}{=} \PYG{l+m+mi}{0}
                \PYG{n}{tries} \PYG{o}{=} \PYG{l+m+mi}{0}

            \PYG{c+c1}{\PYGZsh{} OPTION 1: clear the tree every time}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree} \PYG{o}{=} \PYG{n}{MCTS}\PYG{p}{(}\PYG{n}{board}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{board}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} OPTION 2: if average agent decision time is too long, clear the MCTS tree}
            \PYG{c+c1}{\PYGZsh{} if sum(agent\PYGZus{}decision\PYGZus{}times) / len(agent\PYGZus{}decision\PYGZus{}times) \PYGZgt{} 5:}
            \PYG{c+c1}{\PYGZsh{}     self.tree = MonteCarloTreeSearch(board=self.board)}
            \PYG{c+c1}{\PYGZsh{}     agent\PYGZus{}decision\PYGZus{}times = []}

            \PYG{n}{progress\PYGZus{}bar}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}\PYGZsq{}}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} load tree with MonteCarloSearchDecoder}
    \PYG{c+c1}{\PYGZsh{} with open(\PYGZsq{}progress.json\PYGZsq{}, \PYGZsq{}r\PYGZsq{}) as f:}
    \PYG{c+c1}{\PYGZsh{}     tree = decode\PYGZus{}tree(json.load(f))}
    \PYG{n}{qplayer} \PYG{o}{=} \PYG{n}{QLearningPlayer}\PYG{p}{()}
    \PYG{n}{qplayer}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
\end{Verbatim}
